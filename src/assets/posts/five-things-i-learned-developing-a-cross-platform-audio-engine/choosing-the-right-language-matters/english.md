A significant early question was where the shared audio logic should live and what it should be written in. Kick Snare Hat Apps already shares higher-level application code using [Kotlin Multiplatform](https://kotlinlang.org/docs/multiplatform/kmp-overview.html), which works well for a lot of non–real-time logic, but that approach starts to break down once timing guarantees become important.

On Android in particular, the fact that the primary audio API is written in C++ introduces an extra boundary. Calling into it from Kotlin requires going through JNI, and more importantly, the garbage-collected environment of the JVM makes it near impossible to reason about timing-sensitive behavior.

Web and iOS have different execution models where integrating generated Kotlin Multiplatform code into their audio paths is technically possible, it would yield diverging paths in a project that is otherwise aiming to unify as much shared implementation as possible.

That left a relatively small set of options for the core audio engine. C++ is the obvious choice in this space, but Rust had been gaining traction as a systems language with strong guarantees around memory safety, performance, developer ergonomics, as well as a C-compatible [ABI](https://en.wikipedia.org/wiki/Application_binary_interface). While it wasn’t widely used in mainstream audio applications at the time either, it offered an opportunity to work in an already familiar domain with a new language.

Once Rust was chosen for the core, it became clear that the interface between the core and the native layers needed to be treated as part of the design rather than an implementation detail. Decisions about how memory is allocated, how instances are created and destroyed, how state is passed back and forth, and how macros could be leveraged to reduce [FFI](https://en.wikipedia.org/wiki/Foreign_function_interface) boilerplate ended up being just as important as the audio processing logic that Rust itself would be responsible for.
